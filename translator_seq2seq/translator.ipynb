{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9666a57c-e2d5-474f-b499-0ba7a1ee37c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torchtext.legacy.datasets import Multi30k\n",
    "# from torchtext.legacy.data import Field, BucketIterator\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "# from torchtext.vocab import Vocab\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828221d8-eb78-4281-88dc-6a1e2ee6ae44",
   "metadata": {},
   "source": [
    "We'll create the tokenizers. A tokenizer is used to turn a string containing a sentence into a list of individual tokens that make up that string, e.g. \"good morning!\" becomes [\"good\", \"morning\", \"!\"]. the models must first be downloaded using the following on the command line:\n",
    "* python -m spacy download en_core_web_sm\n",
    "* python -m spacy download de_core_news_sm\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59e456cc-e375-4a47-a234-c39f83d0d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d91eb-8910-4427-a96e-0432d406e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea932e-87f5-422d-bcf8-c05dbc261120",
   "metadata": {},
   "source": [
    "torchtext's Fields handle how data should be processed.We set the tokenize argument to the correct tokenization function for each, with German being the SRC (source) field and English being the TRG (target) field. The field also appends the \"start of sequence\" and \"end of sequence\" tokens via the init_token and eos_token arguments, and converts all words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "476b12bc-f971-4008-a039-386a93863b49",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext' has no attribute 'legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m SRC \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mField(tokenize \u001b[38;5;241m=\u001b[39m tokenize_de, \n\u001b[0;32m      2\u001b[0m             init_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m             eos_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      4\u001b[0m             lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchtext' has no attribute 'legacy'"
     ]
    }
   ],
   "source": [
    "SRC = torchtext.legacy.data.Field(tokenize = tokenize_de, \n",
    "            init_token = '', \n",
    "            eos_token = '', \n",
    "            lower = True)\n",
    "\n",
    "# TRG = data.Field(tokenize = tokenize_en, \n",
    "#             init_token = '', \n",
    "#             eos_token = '', \n",
    "#             lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ab59cb9-bdc5-482a-a3ba-3b280b97189f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'splits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data, valid_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMulti30k\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m(exts \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.de\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.en\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[0;32m      2\u001b[0m                                                     fields \u001b[38;5;241m=\u001b[39m (SRC, TRG))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'splits'"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = datasets.Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301afde-40de-4980-9fca-b24783399386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
