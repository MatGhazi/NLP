{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287c6e20-4684-41e7-8d55-6b83e8f7f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils import *\n",
    "from data_utils import Vocabulary, tokenizer\n",
    "from train_utils import train\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy\n",
    "# setup\n",
    "NLP = spacy.load('en_core_web_sm')  # NLP toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4ab9c2-5b5f-4c72-9c18-03ed8b9860e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sample</th>\n",
       "      <th>id</th>\n",
       "      <th>sign</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>589736</td>\n",
       "      <td>Aries</td>\n",
       "      <td>Much funny.  2 points.  As mentioned in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>589736</td>\n",
       "      <td>Aries</td>\n",
       "      <td>Harpers, Harpers, everywhere.  Harpers, Har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>589736</td>\n",
       "      <td>Aries</td>\n",
       "      <td>In an earlier post, Johnathan said:   'And ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>589736</td>\n",
       "      <td>Aries</td>\n",
       "      <td>I'd post this on the RTG Blog, but I can't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>589736</td>\n",
       "      <td>Aries</td>\n",
       "      <td>The answer to the first question lies with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sample      id   sign  \\\n",
       "0           1       1  589736  Aries   \n",
       "1           2       2  589736  Aries   \n",
       "2           3       3  589736  Aries   \n",
       "3           4       4  589736  Aries   \n",
       "4           5       5  589736  Aries   \n",
       "\n",
       "                                                text  \n",
       "0     Much funny.  2 points.  As mentioned in the...  \n",
       "1     Harpers, Harpers, everywhere.  Harpers, Har...  \n",
       "2     In an earlier post, Johnathan said:   'And ...  \n",
       "3      I'd post this on the RTG Blog, but I can't...  \n",
       "4     The answer to the first question lies with ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "story = pd.read_csv('./blogtext_short.csv')\n",
    "story.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57781219-ac0a-4f99-91d8-80c21d4e5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# story['id'].value_counts()\n",
    "# story.dtypes\n",
    "story['id'] = story['id'].astype(str)\n",
    "# story.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f5be05-d5dc-4d67-b9e0-296f6abb746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'  # special symbol we use for padding text\n",
    "UNK = '<unk>'  # special symbol we use for rare or unknown word\n",
    "# parameters\n",
    "max_len = 200\n",
    "min_count = 10\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49fe6652-66fd-4c4e-a9fd-0a6b278749cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset():\n",
    "    \n",
    "    # def __init__(self, split='train',  vocab_path='vocab.pkl', max_len=100, min_count=10, data, tokenizer=None):   \n",
    "    def __init__(self, split,  vocab_path, max_len, min_count, data, tokenizer):\n",
    "        # self.path = path\n",
    "        # assert split in ['train', 'test']\n",
    "        self.split = split\n",
    "        self.data = data\n",
    "\n",
    "        self.vocab_path = vocab_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.min_count = min_count\n",
    "        \n",
    "        self.cache = {}\n",
    "        self.vocab = None\n",
    "        \n",
    "        self.classes = []\n",
    "        self.class_to_index = {}\n",
    "        self.text_files = []\n",
    "        \n",
    "#         split_path = f'{path}/{split}'\n",
    "        \n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(data['text'], data['id'], test_size=0.2)\n",
    "        if split == 'test':\n",
    "            red_data = pd.concat([X_test, y_test], axis=1)\n",
    "       \n",
    "        else:\n",
    "            red_data = pd.concat([X_train, y_train], axis=1)\n",
    "        \n",
    "\n",
    "\n",
    "        for cls_idx, label in enumerate(['449628','734562','589736']):  # three labels\n",
    "            text_files = [(fname, cls_idx) for fname in red_data.loc[red_data['id'] == label, 'text']]\n",
    "            self.text_files += text_files\n",
    "            self.classes += [label]\n",
    "            self.class_to_index[label] = cls_idx\n",
    "        \n",
    "        self.num_classes = len(self.classes)\n",
    "            \n",
    "        # build vocabulary from training and validation texts\n",
    "        self.build_vocab()\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # read the tokenized text file and its label (neg=0, pos=1)\n",
    "        fname, class_idx = self.text_files[index]\n",
    "        \n",
    "        if fname in self.cache:\n",
    "            return self.cache[fname], class_idx\n",
    "        \n",
    "        # read text file \n",
    "        text = fname\n",
    "        \n",
    "        # tokenize the text file\n",
    "        tokens = self.tokenizer(text.lower().strip())\n",
    "        \n",
    "        # padding and trimming\n",
    "        if len(tokens) < self.max_len:\n",
    "            num_pads = self.max_len - len(tokens)\n",
    "            tokens = [PAD] * num_pads + tokens\n",
    "        elif len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            \n",
    "        # numericalizing\n",
    "        ids = torch.LongTensor(self.max_len)\n",
    "        for i, word in enumerate(tokens):\n",
    "            if word not in self.vocab.word2index:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # unknown words\n",
    "            elif word != PAD and self.vocab.word2count[word] < self.min_count:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # rare words\n",
    "            else:\n",
    "                ids[i] = self.vocab.word2index[word]\n",
    "                \n",
    "        # save in cache for future use\n",
    "        self.cache[fname] = ids\n",
    "        \n",
    "        return ids, class_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_files)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        if not os.path.exists(self.vocab_path):\n",
    "            vocab = Vocabulary(self.tokenizer)\n",
    "\n",
    "            for line in self.data['text']:\n",
    "                vocab.add_sentence(line.lower())\n",
    "\n",
    "            # sort words by their frequencies\n",
    "            words = [(0, PAD), (0, UNK)]\n",
    "            words += sorted([(c, w) for w, c in vocab.word2count.items()], reverse=True)\n",
    "\n",
    "            self.vocab = Vocabulary(self.tokenizer)\n",
    "            for i, (count, word) in enumerate(words):\n",
    "                self.vocab.word2index[word] = i\n",
    "                self.vocab.word2count[word] = count\n",
    "                self.vocab.index2word[i] = word\n",
    "                self.vocab.count += 1\n",
    "\n",
    "            pickle.dump(self.vocab, open(self.vocab_path, 'wb'))\n",
    "        else:\n",
    "            self.vocab = pickle.load(open(self.vocab_path, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d411a8da-86d6-4f74-89df-a9bbc254fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextClassificationDataset(split='train', vocab_path='vocab_class.pkl', max_len=max_len, min_count=min_count, data=story, tokenizer=tokenizer)\n",
    "                 \n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7970d3-ad30-48a4-a6e6-9173290c0456",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = TextClassificationDataset(split='test', vocab_path='vocab_class.pkl', max_len=max_len, min_count=min_count, data=story, tokenizer=tokenizer)\n",
    "\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4643162-5ea3-4beb-99d8-b555dec2c2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7052"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9245233-53be-4dd1-98df-c7c74248942d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1764"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcac9d5-80fa-4896-b44c-f1afdc8130ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['449628', '734562', '589736']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f5a2637-5f7d-4baf-b95e-d2678a0a0068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'449628': 0, '734562': 1, '589736': 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a472ee52-eb66-4b55-b2dc-3bc80c603983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449628\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,   18,  191,   10,    1,   10,    5,  174,   15,  217, 1030,\n",
      "          13,   17,   50, 1916,    5,  174,   15, 2858,    4, 1481,   15,  121,\n",
      "         619,  620,   97,   17,   76,    5,   10,    1,   10, 4432,  145,  380,\n",
      "          39,   92,   77,   15, 1251,    2, 2858,  429, 5038,   61,   15, 3596,\n",
      "         115,   15,   79, 1656,    9,    2,  661,  140,  131, 1873,    2,   87,\n",
      "        5096,   85,    5,    1,   37, 1481,   15,  266,    5, 2195,  131,    1,\n",
      "           2,   51, 2366,  797,   27,   42,  145,  380,    3,   15,   49,    1,\n",
      "        1481,   11,   15,   82,   31,  802,   15,   67,    2,   42,  380,   49,\n",
      "         174, 3968,   42, 4881,    7, 1481,  147,    2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12692\\1020449359.py:67: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:77.)\n",
      "  ids = torch.LongTensor(self.max_len)\n"
     ]
    }
   ],
   "source": [
    "ids, label = train_ds[0]\n",
    "print(train_ds.classes[label])\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5004f1f7-cd1a-48e2-a9de-b42ee2df48fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> urllink use ' <unk> ' to help you change careers it 's an exercise to help you identify the skills you most enjoy using here 's how to ' <unk> ' gather two friends or other people you trust . identify several instances when you achieved something you were proud of . write down those experiences . then examine them to <unk> what skills you used to achieve those <unk> . by sharing ideas with your two friends , you can <unk> skills that you did n't realize you had . your friends can help confirm your strengths and skills too .\n"
     ]
    }
   ],
   "source": [
    "# convert back the sequence of integers into original text\n",
    "print(' '.join([train_ds.vocab.index2word[i.item()] for i in ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd2add8-18c7-4ab0-aa1d-86365d0aed15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         urlLink Use 'Trioing' to Help You Change Careers    It's an exercise to help you identify the skills you most enjoy using: Here's how to 'trio': Gather two friends or other people you trust.  Identify several instances when you achieved something you were proud of. Write down those experiences. Then examine them to pinpoint what skills you used to achieve those successes.   By sharing ideas with your two friends, you can uncover skills that you didn't realize you had. Your friends can help confirm your strengths and skills too.     \n"
     ]
    }
   ],
   "source": [
    "# print the original text\n",
    "print(train_ds.text_files[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b85d18ab-b40a-491b-b6bd-86e63a5d8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embed_size\n",
    "        self.num_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        # x = [seq_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded = [seq_len, batch_size, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # output = [seq_len, batch_size, hidden_dim * num_directions]\n",
    "        # hidden = [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        # cell = [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "            # hidden = [batch_size, hidden_dim * num_directions]\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "            # hidden = [batch_size, hidden_dim]\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6f08bfc-df77-4468-981c-14afa2d4a098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2295386a-fbff-411c-9a68-52864ab502a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5226\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 2 + len([w for (w, c) in train_ds.vocab.word2count.items() if c >= min_count])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b57c959-b4f4-4b3a-9033-cb0619c07139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\Conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "embed_size = 100\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = train_ds.num_classes\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# Define the model\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff041303-208b-4a35-8373-040098966a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd86d87b-a7e9-4bf7-803c-bf57baee336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "criterion = criterion.to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.7, 0.99))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33f9a911-a2f3-4288-a8ab-d8000ca4ea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  4/ 5] | Training Loss: 0.0135 | Testing Loss: 0.0143 | Training Acc: 65.01 | Testing Acc: 64.46\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce576a6660844617bc2d4b0852c4e17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963d3c27e008431699e6b23c49ab9759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:  5/ 5] | Training Loss: 0.0129 | Testing Loss: 0.0136 | Training Acc: 66.44 | Testing Acc: 64.40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = train(model, train_dl, valid_dl, criterion, optimizer, device, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f7765-5703-453e-b818-c4a943453443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
